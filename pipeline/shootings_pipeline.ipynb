{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/jovyan/stage-f-10-police-shootings/data/out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(download_url, out_path):\n",
    "    import subprocess\n",
    "    import sys\n",
    "    import logging\n",
    "    \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    \n",
    "    default_url = \"https://storage.googleapis.com/police-shootings/data/shootings.csv\"\n",
    "    url = download_url if download_url else default_url\n",
    "    \n",
    "    subprocess.run([\"wget\", \"-O\", f\"{out_path}/shootings.csv\", url])\n",
    "    \n",
    "    print(\"File Downloaded\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Downloaded\n"
     ]
    }
   ],
   "source": [
    "get_data(\"\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(out_path):\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas', 'scikit-learn', 'imblearn'])\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import imblearn\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import shuffle\n",
    "    import logging\n",
    "    import pickle\n",
    "    \n",
    "    \n",
    "    def preprocess(data, out_path, is_train=False):\n",
    "\n",
    "\n",
    "        data['day'] = pd.DatetimeIndex(data['date']).day\n",
    "        data['month'] = pd.DatetimeIndex(data['date']).month\n",
    "        data['year'] = pd.DatetimeIndex(data['date']).year\n",
    "\n",
    "\n",
    "\n",
    "        cat_cols = ['state', 'arms_category']\n",
    "\n",
    "        onehot_encoding_columns = ['gender', 'race', 'signs_of_mental_illness', 'manner_of_death', 'body_camera']\n",
    "\n",
    "        data = pd.get_dummies(data, drop_first=True, columns=onehot_encoding_columns, prefix_sep='-')\n",
    "\n",
    "        if is_train:\n",
    "\n",
    "            state_encoder = LabelEncoder()\n",
    "            ac_encoder = LabelEncoder()\n",
    "\n",
    "            cat_cols_encoders = [state_encoder, ac_encoder]\n",
    "\n",
    "            encoders = zip(cat_cols, cat_cols_encoders)\n",
    "\n",
    "            for column, encoder in encoders:\n",
    "\n",
    "                data[column] = encoder.fit_transform(data[column])\n",
    "\n",
    "                with open(f\"{out_path}/{column}_encoder.pkl\", \"wb\") as enc:\n",
    "                    pickle.dump(encoder, enc)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            encoders_dict = {}\n",
    "            for col in cat_cols:\n",
    "\n",
    "                with open(f\"{out_path}/{col}_encoder.pkl\", \"rb\") as enc:\n",
    "                    encoders_dict[f\"{col}_encoder\"] = pickle.load(enc)\n",
    "\n",
    "\n",
    "            encoders = zip(cat_cols, encoders_dict.keys())\n",
    "\n",
    "            for col, encoder in encoders:\n",
    "                data[col] = encoders_dict[encoder].transform(data[col])\n",
    "\n",
    "\n",
    "        df_copy = data.copy()\n",
    "\n",
    "        df_copy = shuffle(df_copy)\n",
    "        features = df_copy.drop(columns=['name','date','label', 'id', 'day', 'month', 'year',\n",
    "                                         'armed', 'city', 'threat_level', 'flee' ])\n",
    "        target = df_copy['label']\n",
    "\n",
    "\n",
    "        # Oversampling the undersampled labels\n",
    "        if is_train:\n",
    "            smote = SMOTE(random_state=0)\n",
    "            X, y = smote.fit_sample(features, target)\n",
    "\n",
    "        else:\n",
    "            X, y = features, target\n",
    "\n",
    "        # converting ndarray to dataframe\n",
    "        X = pd.DataFrame(X, columns=features.columns)\n",
    "        y = pd.Series(y, name=target.name)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "        \n",
    "    def f(row):\n",
    "        \n",
    "        '''\n",
    "          Function that will be used to create the target column of two classes 1 and 0.\n",
    "          Where 1 represents the unjustified cases and 0 represents the just ones. \n",
    "          '''\n",
    "        if ((row['threat_level']=='undetermined' or row['threat_level']=='other') and (row['flee']=='Not fleeing')):\n",
    "            val = 1\n",
    "\n",
    "        else:\n",
    "            val = 0\n",
    "        return val\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    data = pd.read_csv(f'{out_path}/shootings.csv')\n",
    "    data['label'] = data.apply(f, axis=1)\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=100)\n",
    "    \n",
    "    trainset = preprocess(train, out_path, is_train=True)\n",
    "    testset = preprocess(test, out_path)\n",
    "    \n",
    "    logging.info(f\"Training data count: {trainset[0].shape}\")\n",
    "    logging.info(f\"Testing data count: {testset[0].shape}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    with open(f\"{out_path}/trainset.pkl\", \"wb\") as train:\n",
    "        pickle.dump(trainset, train)\n",
    "        \n",
    "    with open(f\"{out_path}/testset.pkl\", \"wb\") as test:\n",
    "        pickle.dump(testset, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(out_path, trainset):\n",
    "    \n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas', 'scikit-learn'])\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import joblib\n",
    "#     from storage import Storage\n",
    "#     import io\n",
    "#     from io import BytesIO\n",
    "    \n",
    "#     preprocessed_data = pickle.load(f'{out_path}/trainset.pkl')\n",
    "    \n",
    "    with open(f\"{out_path}/trainset.pkl\", 'rb') as f:\n",
    "        preprocessed_data = pickle.load(f)\n",
    "        \n",
    "    features = preprocessed_data[0]\n",
    "    targets = preprocessed_data[1]\n",
    "    \n",
    "    lrc = LogisticRegression()\n",
    "    lrc.fit(features, targets)\n",
    "    \n",
    "    \n",
    "    with open(f'{out_path}/export', \"wb\") as model:\n",
    "        joblib.dump(lrc, model)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(output_dir, 'trainset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(out_path, testset, model_uri):\n",
    "    \n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas', 'scikit-learn'])\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import joblib\n",
    "    import logging\n",
    "    \n",
    "    with open(f\"{out_path}/testset.pkl\", 'rb') as f:\n",
    "        preprocessed_data = pickle.load(f)\n",
    "        \n",
    "    with open(f\"{out_path}/export\", 'rb') as f:\n",
    "        lrc = joblib.load(f)\n",
    "        \n",
    "    features = preprocessed_data[0]\n",
    "    targets = preprocessed_data[1]\n",
    "    \n",
    "    lrc_pred = lrc.predict_proba(features)\n",
    "    auc_score = roc_auc_score(targets, lrc_pred[:,1])\n",
    "    \n",
    "    print(\"AUC Score\", auc_score)\n",
    "    logging.info(f\"ROC Score: {auc_score}\")\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score 0.6088995203922548\n"
     ]
    }
   ],
   "source": [
    "test(output_dir, 'testset.pkl', '/tmp/export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = comp.func_to_container_op(get_data, base_image=\"python:3.7\")\n",
    "\n",
    "prepare_data_op = comp.func_to_container_op(prepare_data, base_image=\"python:3.7-slim\")\n",
    "\n",
    "train_op = comp.func_to_container_op(train, base_image=\"python:3.7-slim\")\n",
    "\n",
    "test_op = comp.func_to_container_op(test, base_image=\"python:3.7-slim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Police Shootings Sales Justification Pipeline\",\n",
    "    description=\"A Machine Learning Pipeline for determining police shootings justification\"\n",
    ")\n",
    "\n",
    "def police_shootings_pipeline(\n",
    "    out_path=\"/mnt\",\n",
    "    trainset=\"trainset.pkl\",\n",
    "    testset=\"testset.pkl\",\n",
    "    model_uri=\"export\",\n",
    "    download_url=\"\",\n",
    "    serving_name=\"ps-server\"\n",
    "    serving_namespace=\"kubeflow\",\n",
    "    serving_export_dir=\"gs://police-shootings/export\",\n",
    "    transform_image=\"gcr.io/kubeflow-292422/police-shootings-processing:latest\"\n",
    "    \n",
    "):\n",
    "    \n",
    "    volume_op = dsl.VolumeOp(\n",
    "        name=\"volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"2Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    download = download_op(download_url, out_path).add_pvolumes({out_path: volume_op.volume})\n",
    "    \n",
    "    prepare_data = prepare_data_op(out_path).add_pvolumes({out_path: download_container.pvolume})\n",
    "    \n",
    "    train = train_op(out_path, trainset).add_pvolumes({out_path: prepare_data_container.pvolume})\n",
    "    \n",
    "    test = test_op(out_path, testset, model_uri).add_pvolumes({out_path: train_container.pvolume})\n",
    "    \n",
    "    kfserving_template = Template(\n",
    "        \"\"\"\n",
    "            {\n",
    "                  \"apiVersion\": \"serving.kubeflow.org/v1alpha2\",\n",
    "                  \"kind\": \"InferenceService\",\n",
    "                  \"metadata\": {\n",
    "                    \"labels\": {\n",
    "                      \"controller-tools.k8s.io\": \"1.0\"\n",
    "                    },\n",
    "                    \"name\": \"$serving_name\",\n",
    "                    \"namespace\": \"$namespace\"\n",
    "                  },\n",
    "                  \"spec\": {\n",
    "                    \"default\": {\n",
    "                      \"predictor\": {\n",
    "                        \"minReplicas\": 1,\n",
    "                        \"serviceAccountName\": \"kf-user\",\n",
    "                        \"sklearn\": {\n",
    "                          \"storageUri\": \"$bucket\"\n",
    "                        }\n",
    "                      },\n",
    "                      \"transformer\": {\n",
    "                        \"serviceAccountName\": \"kf-user\",\n",
    "                        \"minReplicas\": 1,\n",
    "                        \"custom\": {\n",
    "                          \"container\": {\n",
    "                            \"image\": \"$transformer\",\n",
    "                            \"name\": \"user-container\",\n",
    "                            \"imagePullPolicy\": \"Always\"\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    kfservingjson = kfserving_template.substitute({'name': str(serving_name),\n",
    "                                                  'namespace': str(serving_namespace),\n",
    "                                                  'bucket': str(serving_export_dir),\n",
    "                                                  'transform': str(transform_image)})\n",
    "    \n",
    "    kfservingdeployment = json.loads(kfservingjson)\n",
    "    \n",
    "    serve = dsl.ResourceOp(\n",
    "        name=\"serve\",\n",
    "        k8s_resource=kfservingdeployment,\n",
    "        action=\"apply\",\n",
    "        success_condition=status.url\n",
    "    )\n",
    "    \n",
    "    serve.after(test)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = police_shootings_pipeline\n",
    "experiment_name = 'police-shootings-training'\n",
    "run_name = f'{pipeline_func.__name__} run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = '/mnt',\n",
    "TRAINSET = 'trainset.pkl',\n",
    "TESTSET = 'testset.pkl',\n",
    "MODEL_URI = '/tmp/export',\n",
    "DOWNLOAD_URL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arguments = {\n",
    "    \"out_path\": OUT_PATH,\n",
    "    \"trainset\": TRAINSET,\n",
    "    \"testset\": TESTSET,\n",
    "    \"model_uri\": MODEL_URI,\n",
    "    \"download_url\": DOWNLOAD_URL,\n",
    "}\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func, f'{experiment_name}.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/6eab424f-72e2-4f57-b368-822b1f7f481b\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/6cada916-e59e-4542-aff8-cfd215f83fe3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
    "                                                    experiment_name=experiment_name,\n",
    "                                                    run_name=run_name,\n",
    "                                                    arguments={}\n",
    "                                                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
